{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "b894c2bf-461f-462e-b2ba-7369dfa25556"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as sk\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data is a dataframe produced by running \"targets_by_drugs.R\" \n",
    "\n",
    "Each row is a drug. The first five columns are whether the drug is predicted to cause Nausea, Fever, Dyspnoea, Rash or Vomiting. The remaining columns are protein targets. I chose to focus on these five side effects since they were the most common adverse events reported, and so I had more training data available. Both the features (protein targets) and the targets (side effects) are binary: a drug either binds to a particular protein or it doesn't, and a drug either causes a side effect or it doesn't. The feature matrix is quite sparse: see \"targets_by_drugs.R\" for more details. This is a multi-label classification problem.\n",
    "\n",
    "This data was compiled from Drugbank.ca and from Health Canada's Adverse Events Report database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "e951077f-f604-4fb4-bff5-f474d6bd6da4"
    }
   },
   "outputs": [],
   "source": [
    "Multi = pd.read_csv(\"Multi.txt\",  index_col= 0,sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we split the data into test and training sets. I chose a 20% split since that's fairly standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "2576eebf-2aa7-42cd-9d82-0b0bcd0a8771"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 969 training examples and 243 test examples.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Multi_train, Multi_test = train_test_split(Multi, test_size = 0.2, random_state = 41)\n",
    "print(\"We have %i training examples and %i test examples.\" % (len(Multi_train), len(Multi_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split each set into features (X, comprised of protein targets) and targets (y, comprised of side effects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into features and targets\n",
    "y = Multi_train.iloc[:,0:5]\n",
    "X = Multi_train.iloc[:,6:]\n",
    "#Split test dataset too\n",
    "y_test = Multi_test.iloc[:,0:5]\n",
    "X_test = Multi_test.iloc[:,6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the training data into multiple folds. I chose to do 5 folds, which is somewhat arbitrary. Since the data is so sparse I didn't want to do >5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, random_state = 17, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to fit a model to each of our train/cv splits using a for loop. \n",
    "\n",
    "First, we will add interaction terms, since that improves the accuracy of our model. Since adding second-order interaction terms results in tens of thousands of features, we will then reduce our feature count by fitting a model and then removing those features that have a coefficient with an absolute value of < 4. We will then re-fit our multi-label logistic regression classifier to the new set of features.\n",
    "\n",
    "Finally, we will export our model and the information we need to transform our feature matrix to feed into this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize index\n",
    "m_number = 0\n",
    "\n",
    "#Loop through each train/cv split and fit a new model.\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_cv = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "    y_train, y_cv = y.iloc[train_index,:], y.iloc[test_index,:]\n",
    "    \n",
    "    #Add interaction terms\n",
    "    poly = PolynomialFeatures(degree = 2, interaction_only=True)\n",
    "    X_new = pd.DataFrame(poly.fit_transform(X_train))\n",
    "    X_cv_new = pd.DataFrame(poly.fit_transform(X_cv))\n",
    "    X_test_new = pd.DataFrame(poly.fit_transform(X_test))\n",
    "    X_new.columns = poly.get_feature_names(X.columns)\n",
    "    X_cv_new.columns = poly.get_feature_names(X.columns)\n",
    "    X_test_new.columns = poly.get_feature_names(X.columns)\n",
    "    \n",
    "    #Fit model\n",
    "    model = OneVsRestClassifier(sk.LogisticRegression(class_weight='balanced',  C=100))\n",
    "    model = model.fit(X_new, y_train)\n",
    "    \n",
    "    #Feature reduction\n",
    "    #Threshold is a bit arbitrary but I found 4 to be reasonable for not adversely impacting\n",
    "    #predictive power while still avoiding too much over-fitting.\n",
    "    sfm = SelectFromModel(model, threshold=4)\n",
    "    sfm.fit(X_new, y_train)\n",
    "    X_transform = pd.DataFrame(sfm.transform(X_new))\n",
    "    X_transform.columns = X_new.columns[sfm.get_support()]\n",
    "    X_cv_transform = pd.DataFrame(sfm.transform(X_cv_new))\n",
    "    X_cv_transform.columns = X_cv_new.columns[sfm.get_support()]\n",
    "    X_test_transform = pd.DataFrame(sfm.transform(X_test_new))\n",
    "    X_test_transform.columns = X_test_new.columns[sfm.get_support()]\n",
    "    \n",
    "    #Fit model to reduced features\n",
    "    model = OneVsRestClassifier(sk.LogisticRegression(class_weight='balanced',  C=100))\n",
    "    model = model.fit(X_transform, y_train)\n",
    "\n",
    "    \n",
    "    #Pickle the models & feature reduction parameters to store\n",
    "    with open('LogisticRegression_%s.pickle' % m_number, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    with open('poly_%s.pickle' % m_number, 'wb') as f:\n",
    "        pickle.dump(poly, f)\n",
    "    with open('sfm_%s.pickle' % m_number, 'wb') as f:\n",
    "        pickle.dump(sfm, f)\n",
    "    m_number = m_number + 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
